# -*- coding: utf-8 -*-
"""benign/maling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MnVrevWL4HnTafWczF9E48mlA2efdRK0

Goal: Apply any deep learning(transfer learning) technique to classify the data in to benign/malignant. Then, Apply any explainable ai to identify the region of interest.

Step1: load the data
"""

import tensorflow as tf
import numpy as np
import h5py
from tensorflow.keras import Input, layers, models

model_path = '/Users/valonkrasniqi/Projects/Hassan/UNETBreast.h5'
model = tf.keras.models.load_model(model_path)
model.summary()

with h5py.File(model_path, 'r') as f:
    # Print the top-level groups
    print("Keys in the .h5 file:", list(f.keys()))

    # For example, often you will see something like ['layer_names', 'model_weights', 'optimizer_weights']
    for key in f.keys():
        print(f"{key}: {list(f[key].keys())}")

"""This a Unet model: a UNet model is a popular cbb architecture designed primarily for image segmentation. The model works py having a contracting path(encoder) and an expansive path(decoder). Here the encoder reduces spatial dimensions while increasing feature depth. The decoder reconstructs the image by upsampling.
  Unet is effective as it preserves
  spatial information meaning help recover fine detils while downsampling. it also allows end-to end training and works well with small datasets.

"""

def unet_model(input_shape=(256, 256, 1)):
    # Input layer must be named "input_1"
    inputs = tf.keras.layers.Input(shape=input_shape, name="input_1")
    ### Encoder Block 1 ###
    # First convolution block
    c1 = layers.Conv2D(64, (3,3), padding="same", activation=None, name="conv2d")(inputs)
    c1 = layers.BatchNormalization(name="batch_normalization")(c1)
    c1 = layers.Activation("relu", name="activation")(c1)
    c1 = layers.Dropout(0.5, name="dropout")(c1)  # dropout layer 1

    # Second convolution block
    c1 = layers.Conv2D(64, (3,3), padding="same", activation=None, name="conv2d_1")(c1)
    c1 = layers.BatchNormalization(name="batch_normalization_1")(c1)
    c1 = layers.Activation("relu", name="activation_1")(c1)
    c1 = layers.Dropout(0.5, name="dropout_1")(c1)  # dropout layer 2

    p1 = layers.MaxPooling2D((2,2), name="max_pooling2d")(c1)

    ### Encoder Block 2 ###
    c2 = layers.Conv2D(128, (3,3), padding="same", activation=None, name="conv2d_2")(p1)
    c2 = layers.BatchNormalization(name="batch_normalization_2")(c2)
    c2 = layers.Activation("relu", name="activation_2")(c2)
    c2 = layers.Dropout(0.5, name="dropout_2")(c2)  # dropout layer 3

    c2 = layers.Conv2D(128, (3,3), padding="same", activation=None, name="conv2d_3")(c2)
    c2 = layers.BatchNormalization(name="batch_normalization_3")(c2)
    c2 = layers.Activation("relu", name="activation_3")(c2)
    c2 = layers.Dropout(0.5, name="dropout_3")(c2)  # dropout layer 4

    p2 = layers.MaxPooling2D((2,2), name="max_pooling2d_1")(c2)

    ### Encoder Block 3 ###
    c3 = layers.Conv2D(256, (3,3), padding="same", activation=None, name="conv2d_4")(p2)
    c3 = layers.BatchNormalization(name="batch_normalization_4")(c3)
    c3 = layers.Activation("relu", name="activation_4")(c3)
    c3 = layers.Dropout(0.5, name="dropout_4")(c3)  # dropout layer 5

    c3 = layers.Conv2D(256, (3,3), padding="same", activation=None, name="conv2d_5")(c3)
    c3 = layers.BatchNormalization(name="batch_normalization_5")(c3)
    c3 = layers.Activation("relu", name="activation_5")(c3)
    c3 = layers.Dropout(0.5, name="dropout_5")(c3)  # dropout layer 6

    p3 = layers.MaxPooling2D((2,2), name="max_pooling2d_2")(c3)

    ### Encoder Block 4 (Bottleneck - Encoder Side) ###
    c4 = layers.Conv2D(512, (3,3), padding="same", activation=None, name="conv2d_6")(p3)
    c4 = layers.BatchNormalization(name="batch_normalization_6")(c4)
    c4 = layers.Activation("relu", name="activation_6")(c4)
    c4 = layers.Dropout(0.5, name="dropout_6")(c4)  # dropout layer 7

    c4 = layers.Conv2D(512, (3,3), padding="same", activation=None, name="conv2d_7")(c4)
    c4 = layers.BatchNormalization(name="batch_normalization_7")(c4)
    c4 = layers.Activation("relu", name="activation_7")(c4)
    c4 = layers.Dropout(0.5, name="dropout_7")(c4)  # dropout layer 8

    p4 = layers.MaxPooling2D((2,2), name="max_pooling2d_3")(c4)

    ### Bottleneck ###
    c5 = layers.Conv2D(1024, (3,3), padding="same", activation=None, name="conv2d_8")(p4)
    c5 = layers.BatchNormalization(name="batch_normalization_8")(c5)
    c5 = layers.Activation("relu", name="activation_8")(c5)
    c5 = layers.Dropout(0.5, name="dropout_8")(c5)  # dropout layer 9

    c5 = layers.Conv2D(1024, (3,3), padding="same", activation=None, name="conv2d_9")(c5)
    c5 = layers.BatchNormalization(name="batch_normalization_9")(c5)
    c5 = layers.Activation("relu", name="activation_9")(c5)
    c5 = layers.Dropout(0.5, name="dropout_9")(c5)  # dropout layer 10

    ### Decoder Block 1 ###
    u6 = layers.UpSampling2D((2,2), name="up_sampling2d")(c5)
    u6 = layers.Concatenate(name="concatenate")([u6, c4])
    c6 = layers.Conv2D(512, (3,3), padding="same", activation=None, name="conv2d_10")(u6)
    c6 = layers.BatchNormalization(name="batch_normalization_10")(c6)
    c6 = layers.Activation("relu", name="activation_10")(c6)
    c6 = layers.Dropout(0.5, name="dropout_10")(c6)  # dropout layer 11

    c6 = layers.Conv2D(512, (3,3), padding="same", activation=None, name="conv2d_11")(c6)
    c6 = layers.BatchNormalization(name="batch_normalization_11")(c6)
    c6 = layers.Activation("relu", name="activation_11")(c6)
    c6 = layers.Dropout(0.5, name="dropout_11")(c6)  # dropout layer 12

    ### Decoder Block 2 ###
    u7 = layers.UpSampling2D((2,2), name="up_sampling2d_1")(c6)
    u7 = layers.Concatenate(name="concatenate_1")([u7, c3])
    c7 = layers.Conv2D(256, (3,3), padding="same", activation=None, name="conv2d_12")(u7)
    c7 = layers.BatchNormalization(name="batch_normalization_12")(c7)
    c7 = layers.Activation("relu", name="activation_12")(c7)
    c7 = layers.Conv2D(256, (3,3), padding="same", activation=None, name="conv2d_13")(c7)
    c7 = layers.BatchNormalization(name="batch_normalization_13")(c7)
    c7 = layers.Activation("relu", name="activation_13")(c7)

    ### Decoder Block 3 ###
    u8 = layers.UpSampling2D((2,2), name="up_sampling2d_2")(c7)
    u8 = layers.Concatenate(name="concatenate_2")([u8, c2])
    c8 = layers.Conv2D(128, (3,3), padding="same", activation=None, name="conv2d_14")(u8)
    c8 = layers.BatchNormalization(name="batch_normalization_14")(c8)
    c8 = layers.Activation("relu", name="activation_14")(c8)
    c8 = layers.Conv2D(128, (3,3), padding="same", activation=None, name="conv2d_15")(c8)
    c8 = layers.BatchNormalization(name="batch_normalization_15")(c8)
    c8 = layers.Activation("relu", name="activation_15")(c8)

    ### Decoder Block 4 ###
    u9 = layers.UpSampling2D((2,2), name="up_sampling2d_3")(c8)
    u9 = layers.Concatenate(name="concatenate_3")([u9, c1])
    c9 = layers.Conv2D(64, (3,3), padding="same", activation=None, name="conv2d_16")(u9)
    c9 = layers.BatchNormalization(name="batch_normalization_16")(c9)
    c9 = layers.Activation("relu", name="activation_16")(c9)
    c9 = layers.Conv2D(64, (3,3), padding="same", activation=None, name="conv2d_17")(c9)
    c9 = layers.BatchNormalization(name="batch_normalization_17")(c9)
    c9 = layers.Activation("relu", name="activation_17")(c9)

    # Final output layer: segmentation mask with 1 channel and sigmoid activation
    outputs = layers.Conv2D(1, (1,1), padding="same", activation="sigmoid", name="conv2d_18")(c9)

    # Naming the top-level model to match "top_level_model_weights" (as seen in the file)
    model = models.Model(inputs=inputs, outputs=outputs, name="top_level_model_weights")
    return model

# Build the model
model = unet_model(input_shape=(256,256,1))
model.summary()

model.load_weights("UNETBreast.h5")
